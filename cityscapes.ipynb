{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3306366124103876717\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7103525684\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 5477207229946144364\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"MaskRCNN\")\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Directory to load source dataset\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"processed_data/leftImg8bit\")\n",
    "\n",
    "# Directory to load groundtruth dataset\n",
    "MASK_DIR = os.path.join(ROOT_DIR, \"processed_data/gtFine\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"MaskRCNN/mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "TRAINING = True\n",
    "subset = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapeConfig(Config):\n",
    "    \"\"\"Configuration for training on the cityscape dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the cityscape dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cityscape\"\n",
    "\n",
    "    # We use a GPU with 12GB memory.\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1  # 8\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 6000\n",
    "\n",
    "    # Number of validation steRPNps to run at the end of every training epoch.\n",
    "    VALIDATION_STEPS = 300\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet50\"\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 6  # background + 1 shapes\n",
    "\n",
    "    # Input image resing\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    LEARNING_RATE = 0.01\n",
    "config = CityscapeConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size * cols, size * rows))\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapeDataset(utils.Dataset):\n",
    "    '''\n",
    "    load_shapes()\n",
    "    load_image()\n",
    "    load_mask()\n",
    "    '''\n",
    "\n",
    "    def __init__(self, subset, max_images=0):\n",
    "        super(CityscapeDataset, self).__init__(self)\n",
    "        self.subset = subset\n",
    "        self.max_images = max_images\n",
    "\n",
    "    def load_shapes(self):\n",
    "        \"\"\"\n",
    "        subset: \"train\"/\"val\"\n",
    "        image_id: use index to distinguish the images.\n",
    "        gt_id: ground truth(mask) id.\n",
    "        height, width: the size of the images.\n",
    "        path: directory to load the images.\n",
    "        \"\"\"\n",
    "        # Add classes you want to train\n",
    "        self.add_class(\"cityscape\", 1, \"sidewalk\")\n",
    "        self.add_class(\"cityscape\", 2, \"person\")\n",
    "        self.add_class(\"cityscape\", 3, \"rider\")\n",
    "        self.add_class(\"cityscape\", 4, \"car\")\n",
    "        self.add_class(\"cityscape\", 5, \"truck\")\n",
    "        self.add_class(\"cityscape\", 6, \"bus\")\n",
    "\n",
    "        # Add images\n",
    "        image_dir = \"{}/{}\".format(DATA_DIR, self.subset)\n",
    "        image_ids = os.listdir(image_dir)\n",
    "        if self.max_images != 0:\n",
    "            image_ids = image_ids[:self.max_images]\n",
    "        \n",
    "        for index, item in tqdm(enumerate(image_ids), desc='preparing dataset'):\n",
    "            temp_image_path = \"{}/{}\".format(image_dir, item)\n",
    "            # print(temp_image_path)\n",
    "            temp_image_size = skimage.io.imread(temp_image_path).shape\n",
    "            self.add_image(\"cityscape\", image_id=index, gt_id=os.path.splitext(item)[0],\n",
    "                            height=temp_image_size[0], width=temp_image_size[1],\n",
    "                            path=temp_image_path)\n",
    "\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Load images according to the given image ID.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        image = skimage.io.imread(info['path'])\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"cityscape\":\n",
    "            return info[\"cityscape\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks of the given image ID.\n",
    "        count: the number of masks in each image.\n",
    "        class_id: the first letter of each mask file's name.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        gt_id = info['gt_id']\n",
    "        mask_dir = \"{}/{}/{}\".format(MASK_DIR, self.subset, gt_id)\n",
    "        print(mask_dir)\n",
    "        masks_list = os.listdir(mask_dir)\n",
    "        count = len(masks_list)\n",
    "        mask = np.zeros([info['height'], info['width'], count])\n",
    "        class_ids = []\n",
    "\n",
    "        for index, item in enumerate(masks_list):\n",
    "            temp_mask_path = \"{}/{}\".format(mask_dir, item)\n",
    "            tmp_mask = 255 - skimage.io.imread(temp_mask_path)[:, :, np.newaxis]\n",
    "            mask[:, :, index:index+1] = tmp_mask\n",
    "            class_ids.append(item[0])\n",
    "\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        return mask, np.array(class_ids, dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d2062fa08e49988008aef92871d978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing dataset: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d5e163d3a7498e8bd966aa3bf97066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing dataset: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # Training dataset\n",
    "    dataset_train = CityscapeDataset(\"train\", 100)\n",
    "    dataset_train.load_shapes()\n",
    "    dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = CityscapeDataset(\"val\", 100)\n",
    "dataset_val.load_shapes()\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 100\n",
      "Class Count: 7\n",
      "  0. BG                                                \n",
      "  1. sidewalk                                          \n",
      "  2. person                                            \n",
      "  3. rider                                             \n",
      "  4. car                                               \n",
      "  5. truck                                             \n",
      "  6. bus                                               \n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(\"Image Count: {}\".format(len(dataset_train.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_train.num_classes))\n",
    "for i, info in enumerate(dataset_train.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "# Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "# for image_id in image_ids:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     print('image_id', image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids,\n",
    "#                                 dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0epoch [00:00, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.01\n",
      "\n",
      "Checkpoint Path: c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\logs\\cityscape20211112T2341\\mask_rcnn_cityscape_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "  0%|          | 0/3 [00:00<?, ?epoch/s]ERROR:root:Error processing image {'id': 79, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000079_000019_leftImg8bit.png', 'gt_id': 'aachen_000079_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000079_000019_leftImg8bit'\n",
      "ERROR:root:Error processing image {'id': 11, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000011_000019_leftImg8bit.png', 'gt_id': 'aachen_000011_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000011_000019_leftImg8bit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000079_000019_leftImg8bit\n",
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000011_000019_leftImg8bit\n",
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000024_000019_leftImg8bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 24, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000024_000019_leftImg8bit.png', 'gt_id': 'aachen_000024_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000024_000019_leftImg8bit'\n",
      "ERROR:root:Error processing image {'id': 42, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000042_000019_leftImg8bit.png', 'gt_id': 'aachen_000042_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000042_000019_leftImg8bit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000042_000019_leftImg8bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 71, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000071_000019_leftImg8bit.png', 'gt_id': 'aachen_000071_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000071_000019_leftImg8bit'\n",
      "ERROR:root:Error processing image {'id': 75, 'source': 'cityscape', 'path': 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/leftImg8bit/train/aachen_000075_000019_leftImg8bit.png', 'gt_id': 'aachen_000075_000019_leftImg8bit', 'height': 1024, 'width': 2048}\n",
      "Traceback (most recent call last):\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1712, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"MaskRCNN\\mrcnn\\model.py\", line 1215, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"<ipython-input-19-fcc51be62486>\", line 67, in load_mask\n",
      "    masks_list = os.listdir(mask_dir)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000075_000019_leftImg8bit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000071_000019_leftImg8bit\n",
      "c:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\processed_data/gtFine/train/aachen_000075_000019_leftImg8bit\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000075_000019_leftImg8bit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4a7803d4c20c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mcustom_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 )\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\MaskRCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources, verbose)\u001b[0m\n\u001b[0;32m   2376\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2378\u001b[1;33m             \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2379\u001b[0m         )\n\u001b[0;32m   2380\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mrcnn\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\MaskRCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdata_generator\u001b[1;34m(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   1710\u001b[0m                     load_image_gt(dataset, config, image_id, augment=augment,\n\u001b[0;32m   1711\u001b[0m                                 \u001b[0maugmentation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m                                 use_mini_mask=config.USE_MINI_MASK)\n\u001b[0m\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[1;31m# Skip images that have no instances. This can happen in cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Soham\\Dropbox (ASU)\\ASU_StudyMaterials\\Fall21\\SML\\Project\\mask_r_cnn_sml\\MaskRCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[1;34m(dataset, config, image_id, augment, augmentation, use_mini_mask)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     \u001b[1;31m# Load image and mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m     \u001b[0moriginal_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m     image, window, scale, padding, crop = utils.resize_image(\n",
      "\u001b[1;32m<ipython-input-19-fcc51be62486>\u001b[0m in \u001b[0;36mload_mask\u001b[1;34m(self, image_id)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mmask_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{}/{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMASK_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mmasks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'height'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'width'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\Soham\\\\Dropbox (ASU)\\\\ASU_StudyMaterials\\\\Fall21\\\\SML\\\\Project\\\\mask_r_cnn_sml\\\\processed_data/gtFine/train/aachen_000075_000019_leftImg8bit'"
     ]
    }
   ],
   "source": [
    "\n",
    "if TRAINING:\n",
    "    # tqdm bar for tracking training progress\n",
    "    callback = TqdmCallback()\n",
    "    callback.display()\n",
    "    \n",
    "    # Train the head branches\n",
    "    # Passing layers=\"heads\" freezes all layers except the head\n",
    "    # layers. You can also pass a regular expression to select\n",
    "    # which layers to train by name pattern.\n",
    "    # model.train(dataset_train, dataset_val,\n",
    "    #             learning_rate=config.LEARNING_RATE,\n",
    "    #             epochs=1,\n",
    "    #             layers='heads')\n",
    "\n",
    "    # Fine tune all layers\n",
    "    # Passing layers=\"all\" trains all layers. You can also\n",
    "    # pass a regular expression to select which layers to\n",
    "    # train by name pattern.\n",
    "    # learning_rate = 0.01\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=3,\n",
    "                layers=\"all\",\n",
    "                custom_callbacks = [callback],\n",
    "                verbose=0\n",
    "                )\n",
    "\n",
    "    callback = TqdmCallback()\n",
    "    callback.display()\n",
    "    # learning_rate = 0.001\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE / 10,\n",
    "                epochs=4,\n",
    "                layers=\"all\",\n",
    "                custom_callbacks = [callback],\n",
    "                verbose=0\n",
    "                )\n",
    "\n",
    "    # Save weights\n",
    "    # Typically not needed because callbacks save after every epoch\n",
    "    # Uncomment to save manually\n",
    "    # model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "    # model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(CityscapeConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_MIN_DIM = 1024\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\",\n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "\n",
    "model.load_weights(model_path, by_name=True)\n",
    "\n",
    "# Test on random images\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 2)\n",
    "for image_id in image_ids:\n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "\n",
    "\t# log(\"original_image\", original_image)\n",
    "\t# log(\"image_meta\", image_meta)\n",
    "\t# log(\"gt_class_id\", gt_class_id)\n",
    "\t# log(\"gt_bbox\", gt_bbox)\n",
    "\t# log(\"gt_mask\", gt_mask)\n",
    "\n",
    "    # visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n",
    "    #                         dataset_val.class_names, figsize=(8, 8))\n",
    "\n",
    "    results = model.detect([original_image], verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n",
    "                                dataset_val.class_names, r['scores'], ax=get_ax())\n",
    "\n",
    "\n",
    "# Compute mAP\n",
    "#APs_05: IoU = 0.5\n",
    "#APs_all: IoU from 0.5-0.95 with increments of 0.05\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 489)\n",
    "APs_05 = []\n",
    "APs_all = []\n",
    "\n",
    "for image_id in image_ids:\n",
    "    # Load images and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP_05, precisions, recalls, overlaps = \\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs_05.append(AP_05)\n",
    "\n",
    "    AP_all = \\\n",
    "        utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs_all.append(AP_all)\n",
    "\n",
    "print(\"mAP: \", np.mean(APs_05))\n",
    "print(\"mAP: \", np.mean(APs_all))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c6ce1abcb17969d526c69f42109af510ec32086f3093d0501e25cf11b66b2a8"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('matterport': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
